{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxr-xr-x  4 sjham sjham 4096 Jan 27 23:24 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 14 sjham sjham 4096 Jan 27 21:53 \u001b[01;34m..\u001b[0m/\r\n",
      "drwxr-xr-x  2 sjham sjham 4096 Jan 27 23:19 \u001b[01;34mdata_set\u001b[0m/\r\n",
      "-rw-r--r--  1 sjham sjham  344 Jan 27 22:03 matcher.py\r\n",
      "-rw-r--r--  1 sjham sjham  412 Jan 27 22:57 match_to_col.py\r\n",
      "drwxr-xr-x  2 sjham sjham 4096 Jan 27 23:24 \u001b[01;34m__pycache__\u001b[0m/\r\n",
      "-rw-r--r--  1 sjham sjham  319 Jan 27 23:14 reader.py\r\n",
      "-rw-r--r--  1 sjham sjham  250 Jan 27 23:00 save.py\r\n",
      "-rw-r--r--  1 sjham sjham  392 Jan 27 21:59 tokenizer.py\r\n",
      "-rw-r--r--  1 sjham sjham  364 Jan 27 22:56 vecterizer.py\r\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "%ls -al\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('broadcast.xlsx') \n",
    "# data = pd.read_csv('manual.csv') \n",
    "raw_data = pd.read_csv('kbs_2016,3,2_to_2016,4,30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunksize = 500\n",
    "# chunks = []\n",
    "# for chunk in pd.read_csv('pizza.csv', chunksize=chunksize):\n",
    "#     # Do stuff...\n",
    "#     chunks.append(chunk)\n",
    "\n",
    "# df = pd.concat(chunks, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# raw_data['date'] = pd.to_datetime(raw_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data['date'] = raw_data['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['date'] = '2016.' + data['date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['date'] = data['date'].str.replace('/', '.').str.replace('-', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['date']= pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.set_index('date')\n",
    "# raw_data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['title_refined'] = data.제목.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "# raw_data['title_refined']= raw_data.title.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['title_refined'] = data['title_refined'].str.strip()\n",
    "# raw_data['title_refined'] = raw_data['title_refined'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.isnull().sum()\n",
    "# raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.dropna(subset=['title_refined'],axis=0, how='any')\n",
    "# data = data.dropna(subset=['제목'],axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop('title',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ngrams(string, n=3):\n",
    "# #     string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "#     ngrams = zip(*[string[i:] for i in range(n)])\n",
    "#     return [''.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tagging(x):\n",
    "#     return mecab.nouns(x)\n",
    "#     return x+'1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tagging(x):\n",
    "#     return mecab.morphs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sjham/project/nlp/preprocessing/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/preprocess.py'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import os\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "mecab = Mecab()\n",
    "\n",
    "\n",
    "#reading file\n",
    "\n",
    "def read_file(file1, file2):\n",
    "    if 'xlsx' in str(file1): \n",
    "        data1 = pd.read_excel(str(file1))\n",
    "    else:\n",
    "        data1 = pd.read_csv(str(file1))\n",
    "    if 'xlsx' in str(file2):\n",
    "        data2 = pd.read_excel(str(file2))\n",
    "    else:\n",
    "        data2 = pd.read_csv(str(file2))\n",
    "    return data1, data2\n",
    "\n",
    "\n",
    "#cleaning data\n",
    "\n",
    "def cleaning_data(data1, data2):\n",
    "    data1['title_refined'] = data1.제목.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "    data2['title_refined'] = data2.title.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "    data1['title_refined'] = data1['title_refined'].str.strip()\n",
    "    data2['title_refined'] = data2['title_refined'].str.strip()\n",
    "    return (data1, data2)\n",
    "\n",
    "\n",
    "#tokenizing\n",
    "\n",
    "def ngrams(string, num_ngram): \n",
    "#     string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(num_ngram)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "# print(ngrams(string))\n",
    "\n",
    "def tagging_morphs(string):\n",
    "    return mecab.morphs(string)\n",
    "\n",
    "def tagging_nouns(string):\n",
    "    return mecab.nouns(string)\n",
    "\n",
    "\n",
    "\n",
    "# finding matching data\n",
    "\n",
    "def intersect(data1, data2):\n",
    "    inter = []\n",
    "    for i in data1:\n",
    "        for j in data2:\n",
    "            inter.append(len(set(i).intersection(set(j))))\n",
    "    return inter\n",
    "\n",
    "def similar(data1, data2):\n",
    "    sim=[]\n",
    "#     threshold = 0.4\n",
    "    for i in data1:\n",
    "        for j in data2:\n",
    "            t.append(SequenceMatcher(None, i, j).ratio())\n",
    "    return sim\n",
    "\n",
    "\n",
    "#vecterize and send to columns\n",
    "\n",
    "def vecterize_to_cols(data1_col_tagged, data2_col_tagged):\n",
    "    vect = np.array(similar(data1_col_tagged, data2_col_tagged)).reshape(len(data1.shape),len(data2.shape))\n",
    "    arg_count = np.argmax(vect, axis=1)\n",
    "    max_count = np.max(vect, axis=1)\n",
    "    data1['arg_count']=pd.Series(arg_count)\n",
    "    data1['max_count']=pd.Series(max_count)\n",
    "    return\n",
    "\n",
    "\n",
    "#matched data to columns\n",
    "\n",
    "def match_to_cols(data1, data2):\n",
    "    matching_title= []\n",
    "    matching_aT = []\n",
    "    t = int(data2.columns.get_loc('title'))\n",
    "    aT = int(data2.columns.get_loc('articleText')) \n",
    "    for i in data1.arg_count:\n",
    "        matching_title.append(data2.iloc[i,t])\n",
    "        matching_aT.append(data2.iloc[i,aT])\n",
    "    data1['matching_title'] = matching_title\n",
    "    data1['matching_aT'] = matching_aT\n",
    "    return\n",
    "\n",
    "\n",
    "#save to file\n",
    "\n",
    "def save_to_file(data1, file_name):\n",
    "    pd.to_pickle(data1, file_name+'.pkl')\n",
    "    data['verify'] = 0\n",
    "    file_name = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "    file_name.to_csv(file_name+'.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sjham/project/nlp/preprocessing/reader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/reader.py'\n",
    "import pandas as pd\n",
    "\n",
    "def read_file(file1, file2):\n",
    "    if 'xlsx' in str(file1): \n",
    "        data1 = pd.read_excel(str(file1))\n",
    "    else:\n",
    "        data1 = pd.read_csv(str(file1))\n",
    "    if 'xlsx' in str(file2):\n",
    "        data2 = pd.read_excel(str(file2))\n",
    "    else:\n",
    "        data2 = pd.read_csv(str(file2))\n",
    "    return (data1, data2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sjham/project/nlp/preprocessing/cleaner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/cleaner.py'\n",
    "import re\n",
    "def cleaning_data(data1, data2):\n",
    "    data1['title_refined'] = data1.제목.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "    data2['title_refined'] = data2.title.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "    data1['title_refined'] = data1['title_refined'].str.strip()\n",
    "    data2['title_refined'] = data2['title_refined'].str.strip()\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "# data = data.dropna(subset=['제목'],axis=0, how='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sjham/project/nlp/preprocessing/tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/tokenizer.py'\n",
    "import pandas as pd\n",
    "# import os\n",
    "from konlpy.tag import Mecab\n",
    "# import re\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "def ngrams(string, num_ngram): \n",
    "#     string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(num_ngram)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "# print(ngrams(string))\n",
    "\n",
    "def tagging_morphs(string):\n",
    "    return mecab.morphs(string)\n",
    "\n",
    "def tagging_nouns(string):\n",
    "    return mecab.nouns(string)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/matcher.py'\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def intersect(data1, data2):\n",
    "    inter = []\n",
    "    for i in data1:\n",
    "        for j in data2:\n",
    "            inter.append(len(set(i).intersection(set(j))))\n",
    "    return inter\n",
    "\n",
    "def similar(data1, data2):\n",
    "    sim=[]\n",
    "#     threshold = 0.4\n",
    "    for i in data1:\n",
    "        for j in data2:\n",
    "            t.append(SequenceMatcher(None, i, j).ratio())\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/vecterizer.py'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vecterize_to_cols(data1.clo_tagged, data2.col_tagged):\n",
    "    vect = np.array(similar(data1.clo_tagged, data2.col_tagged)).reshape(len(data1.shape),len(data2.shape))\n",
    "    arg_count = np.argmax(vect, axis=1)\n",
    "    max_count = np.max(vect, axis=1)\n",
    "    data1['arg_count']=pd.Series(arg_count)\n",
    "    data1['max_count']=pd.Series(max_count)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/match_to_col.py'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def match_to_cols(data1, data2):\n",
    "    matching_title= []\n",
    "    matching_aT = []\n",
    "    t = int(data2.columns.get_loc('title'))\n",
    "    aT = int(data2.columns.get_loc('articleText')) \n",
    "    for i in data1.arg_count:\n",
    "        matching_title.append(data2.iloc[i,t])\n",
    "        matching_aT.append(data2.iloc[i,aT])\n",
    "    data1['matching_title'] = matching_title\n",
    "    data1['matching_aT'] = matching_aT\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile '~/project/nlp/preprocessing/save.py'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_file(data1, file_name):\n",
    "    pd.to_pickle(data1, file_name+'.pkl')\n",
    "    data['verify'] = 0\n",
    "    file_name = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "    file_name.to_csv(file_name+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########unrefined_morphs, refined_morphs, refined_nouns, ngrams  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##refined\n",
    "data = data.dropna(subset=['제목'],axis=0, how='any')\n",
    "\n",
    "data['title_refined'] = data.제목.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "raw_data['title_refined']= raw_data.title.str.replace(r'[/W\\”\\“\\‘\\,\\’\\\"\\'\\.\\∙\\…\\[\\]\\(\\)]+', \" \")\n",
    "\n",
    "data['title_refined'] = data['title_refined'].str.strip()\n",
    "raw_data['title_refined'] = raw_data['title_refined'].str.strip()\n",
    "\n",
    "\n",
    "\n",
    "def ngrams(string, n=3): \n",
    "#     string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "# print(ngrams(string))\n",
    "\n",
    "def tagging_morphs(x):\n",
    "    return mecab.morphs(x)\n",
    "\n",
    "def tagging_nouns(x):\n",
    "    return mecab.nouns(x)\n",
    "\n",
    "data['title_tagged'] = data['title_refined'].apply(ngrams)\n",
    "raw_data['title_tagged'] = raw_data['title_refined'].apply(ngrams)\n",
    "\n",
    "# data['title_tagged'] = data['제목'].apply(tagging_morphs)\n",
    "# raw_data['title_tagged'] = raw_data['title'].apply(tagging_morphs)\n",
    "\n",
    "# data['title_tagged'] = data['title_refined'].apply(tagging_nouns)\n",
    "# raw_data['title_tagged'] = raw_data['title_refined'].apply(tagging_nouns)\n",
    "\n",
    "\n",
    "\n",
    "data = data[0:198]\n",
    "\n",
    "def intersect(a, b):\n",
    "    inter = []\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            inter.append(len(set(i).intersection(set(j))))\n",
    "    return inter\n",
    "\n",
    "# intersect(data.title_tagged, raw_data.title_tagged)\n",
    "\n",
    "# y= []\n",
    "# for i in data.title_tagged:\n",
    "#     for j in raw_data.title_tagged:\n",
    "#         y.append(len(set(i).intersection(set(j))))\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "def similar(a, b):\n",
    "    t=[]\n",
    "#     threshold = 0.4\n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            t.append(SequenceMatcher(None, i, j).ratio())\n",
    "            \n",
    "#             > threshold:\n",
    "#             inter.append(len(set(i).intersection(set(j))))\n",
    "#     if SequenceMatcher(None, a, b).ratio() > threshold:\n",
    "#         t.append(SequenceMatcher(None, a, b).ratio())\n",
    "        #     print(t)\n",
    "    return t\n",
    "        \n",
    "la = data.shape[0]\n",
    "lra = raw_data.shape[0]\n",
    "\n",
    "# n = np.array(intersect(data.title_tagged, raw_data.title_tagged)).reshape(la,lra)\n",
    "n = np.array(similar(data.title_tagged, raw_data.title_tagged)).reshape(la,lra)\n",
    "\n",
    "arg_count = np.argmax(n, axis=1)\n",
    "max_count = np.max(n, axis=1)\n",
    "\n",
    "data['arg_count']=pd.Series(arg_count)\n",
    "data['max_count']=pd.Series(max_count)\n",
    "\n",
    "data['articleText'] = 0\n",
    "\n",
    "matching_aT = []\n",
    "matching_title= []\n",
    "for i in data.arg_count:\n",
    "    matching_title.append(raw_data.iloc[i,0])\n",
    "    matching_aT.append(raw_data.iloc[i,5])\n",
    "\n",
    "data['matching_title'] = matching_title\n",
    "data['matching_aT'] = matching_aT\n",
    "\n",
    "# pd.to_pickle(data, 'refined_nouns.pkl')\n",
    "# pd.to_pickle(data, 'unrefined_morphs.pkl')\n",
    "# pd.to_pickle(data, 'refined_morphs.pkl')\n",
    "pd.to_pickle(data, 'refined_3grams_similar.pkl')\n",
    "# pd.to_pickle(data, 'refined_4grams.pkl')\n",
    "# pd.to_pickle(data, 'refined_4grams_similar.pkl')\n",
    "# pd.to_pickle(data, 'refined_morphs_similar.pkl')\n",
    "# pd.to_pickle(data, 'refined_nouns_similar.pkl')\n",
    "\n",
    "\n",
    "# refined_nouns = data[['제목', 'matching_title', 'max_count', 'matching_aT']]\n",
    "# refined_nouns.to_csv('refined_nouns.csv')\n",
    "\n",
    "# unrefined_morphs = data[['제목', 'matching_title', 'max_count', 'matching_aT']]\n",
    "# unrefined_morphs.to_csv('unrefined_morphs.csv')\n",
    "\n",
    "# refined_morphs = data[['제목', 'matching_title', 'max_count', 'matching_aT']]\n",
    "# refined_morphs.to_csv('refined_morphs.csv')\n",
    "\n",
    "# data['verify'] = 0\n",
    "# refined_4grams = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "# refined_4grams.to_csv('refined_4grams.csv')\n",
    "\n",
    "# data['verify'] = 0\n",
    "# refined_4grams_similar = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "# refined_4grams_similar.to_csv('refined_4grams_similar.csv')\n",
    "\n",
    "# data['verify'] = 0\n",
    "# refined_morphs_similar = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "# refined_morphs_similar.to_csv('refined_morphs_similar.csv')\n",
    "\n",
    "# data['verify'] = 0\n",
    "# refined_nouns_similar = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "# refined_nouns_similar.to_csv('refined_nouns_similar.csv')\n",
    "\n",
    "data['verify'] = 0\n",
    "refined_3grams_similar = data[['제목', 'matching_title', 'verify','max_count', 'matching_aT']]\n",
    "refined_3grams_similar.to_csv('refined_3grams_similar.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refined_nouns_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_2grams.loc[refined_2grams.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_4grams.loc[refined_4grams.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_3grams.loc[refined_3grams.max_count <= 4, :].to_csv('refined_3grams_under4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_3grams.loc[refined_3grams.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_morphs.loc[refined_morphs.max_count <= 4, :].to_csv('refined_morphs_under4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_morphs.loc[refined_morphs.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_nouns.loc[refined_nouns.max_count <= 4, :].to_csv('refined_nouns_under4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_nouns.loc[refined_nouns.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_morphs.loc[unrefined_morphs.max_count <= 4, :].to_csv('unrefined_morphs_under4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_morphs.loc[unrefined_morphs.max_count <= 4, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrefined_morphs.loc[unrefined_morphs.max_count <= 4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '치킨 게임으로 비화된 한일 초계기 갈등…제동장치 없나'\n",
    "#       ['일본 인플루엔자 한국 영향 촉각… 백신 접종 당부'],\n",
    "#       ['미 최장 셧다운 일단 수습…북·미 정상회담 준비 집중'],\n",
    "#       ['자리 양보받으면 기분 나빠…58년 개띠 新중년'],\n",
    "#       ['일요일 아침까지 추워…체감 영하 10도 안팎']]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=3): \n",
    "#     string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "print(ngrams(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=3):\n",
    "    for st in string:\n",
    "        ngrams = zip(*[str(st)[i:] for i in range(n)])\n",
    "        for ngram in ngrams:\n",
    "            return ngram\n",
    "#         return [''.join(ngram) for ngram in ngrams]\n",
    "print(ngrams(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['title_tagged'] = data['title_refined'].apply(tagging).copy()\n",
    "data['title_tagged'] = data['제목'].apply(tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['title_set']= [set(item) for item in data['title_tagged']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data['title_tagged'] = raw_data['title_refined'].apply(tagging)\n",
    "raw_data['title_tagged'] = raw_data['title'].apply(tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data['title_set']= [set(item) for item in raw_data['title_tagged']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_tagged = raw_data.drop(['title_refined', 'title_set'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data_tagged.to_csv('raw_data_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tagged = data.drop(['title_refined', 'title_set'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tagged.to_csv('data_tagged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['사랑', '암', '온정', '시작'], ['차량', '위반','암행', '단속','사랑','운행'],['암행', '시작', '행성']]\n",
    "b = [['무제한', '위반', '온정'] ,['사랑', '암', '나를', '온정'],['토론','단속', '시작', '끝'], ['테러', '방지법'], ['등', '운행', '처리','사죄', '무제한', '암행','차량']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def similar(a, b):\n",
    "    t=[]\n",
    "#     threshold = 0.4\n",
    "    for i in a:\n",
    "        str(i)\n",
    "        for j in b:\n",
    "            str(j)\n",
    "            t.append(SequenceMatcher(None, i, j).ratio())\n",
    "print(similar(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'manual':a})\n",
    "# df2 =pd.DataFrame({'crawl':b}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data[0:198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y= []\n",
    "for i in data.title_tagged:\n",
    "    for j in raw_data.title_tagged:\n",
    "        y.append(len(set(i).intersection(set(j))))\n",
    "len(y)\n",
    "y\n",
    "\n",
    "la = data.shape[0]\n",
    "lra = raw_data.shape[0]\n",
    "\n",
    "import numpy as np\n",
    "n = np.array(y).reshape(la,lra)\n",
    "n\n",
    "arg_count = np.argmax(n, axis=1)\n",
    "max_count = np.max(n, axis=1)\n",
    "arg_count\n",
    "max_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['arg_count']=pd.Series(arg_count)\n",
    "data['max_count']=pd.Series(max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['articleText'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df= df.drop(['count','real_count'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df= df.drop(['text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_data = []\n",
    "for i in data.arg_count:\n",
    "    matching_data.append(raw_data.iloc[i,0])\n",
    "data['articleText'] = matching_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[['제목','max_count', 'articleText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(data, 'with_real_noncharccter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(data, 'with_noncharccter.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(data, 'data_tagged_aT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_data = pd.read_pickle('./with_real_noncharccter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncharccter_data_at_max = pickled_data[['제목','max_count', 'articleText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noncharccter_data_at_max.to_csv('noncharccter_data_at_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncharccter_data_at_max.loc[noncharccter_data_at_max.max_count ==4, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data_at_max.to_csv('tagged_data_at_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data_at_max.loc[tagged_data_at_max.max_count ==4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontagged_data_at_max.to_csv('nontagged_data_at_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontagged_data_at_max.loc[nontagged_data_at_max.max_count <= 3, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_data['articleText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['제목_splited'] = [set(str(i).split()) for i in data.제목_refined]\n",
    "raw_data['title_splited'] = [set(str(i).split()) for i in raw_data.title_refined]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = set1.intersection(set2)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "a = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\n",
    "df = pd.DataFrame(a, columns=['one', 'two', 'three'])\n",
    "\n",
    "df['compare'] = np.where((data['date'] == raw_data['date']) & (data['date'] <= df['three'])\n",
    "                     , df['one'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '치킨 게임으로 인플루엔자 비화된 한일 초계기 갈등…제동장치 , 백신 접종 당부'\n",
    "b = '일본 인플루엔자 한국 영향 촉각… 백신 접종 당부'\n",
    "#       ['미 최장 셧다운 일단 수습…북·미 정상회담 준비 집중'],\n",
    "#       ['자리 양보받으면 기분 나빠…58년 개띠 新중년'],\n",
    "#       ['일요일 아침까지 추워…체감 영하 10도 안팎']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def similar(a, b):\n",
    "    t=[]\n",
    "    threshold = 0.4\n",
    "    if SequenceMatcher(None, a, b).ratio() > threshold:\n",
    "        t.append(SequenceMatcher(None, a, b).ratio())\n",
    "#     print(t)\n",
    "    return t\n",
    "print(similar(a, b))        \n",
    "#     print(s)\n",
    "#     return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "SequenceMatcher(None, raw_data.title_tagged, data.title_tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rd in raw_data.title_tagged:\n",
    "    for d in data.title_tagged:\n",
    "        similar(rd, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['제목_splited'] = [set(str(i).split()) for i in data.제목_refined]\n",
    "# raw_data['title_splited'] = [set(str(i).split()) for i in raw_data.title_refined]\n",
    "\n",
    "data['제목_splited'] = [str(i).split() for i in data.제목_refined]\n",
    "raw_data['title_splited'] = [str(i).split() for i in raw_data.title_refined]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_100 = data['제목_splited'][:100] \n",
    "raw_data_100 = raw_data['title_splited'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['시간', '유세', '돌입', '군소정당', '총력전'], ['총선', '이틀', '앞', '5대', '관전', '포인트는?'], \n",
    "     ['반드시', '투표', '비', '예보', '변수'], ['與', '과반', '확보', '절실', '투표로', '야당', '심판','컷오프', '내일 발표'],\n",
    "     ['윤상현 막말', '파문…','컷오프', '내일 발표']]\n",
    "\n",
    "\n",
    "# raw_data['title_splited'] = [','.join(str(i).split()) for i in raw_data.title_refined]\n",
    "# raw_data['title_fil'] = [set(str(i).split()) for i in raw_data.title]\n",
    "\n",
    "# [더민주, 여당, 독주, 저지, 수권, 야당, 지지], [국민의당, 미래, 위한, 싸움, 3번, 선택], \n",
    "#      [우리가, 대안, 정당, 존재감, 부각, 총력], [내일, 선택의, 날, 투, 개표, 준비, 완료],\n",
    "#  [투표용지, 손동작, 안, 돼, 인증샷, 주의점], [오후, 6시, 출구조사, 당선, 예상자, 발표]]\n",
    "b = [['시간', '유세', '돌입', '군소정당', '총력전'],['與', '과반', '확보', '절실'], ['심판','컷오프', '내일 발표'],\n",
    "     ['yyyyyyyy','ttttt','wswwewe'],['dwddwdfwfw','gfhfghhth','hytyyhtyhytyh']]\n",
    "\n",
    "    \n",
    "    \n",
    "#     [더민주, 여당, 독주, 저지, 수권, 야당, 지지], [국민의당, 미래, 위한, 싸움, 3번, 선택], \n",
    "#      [우리가, 대안, 정당, 존재감, 부각, 총력],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['날짜_in'] = data['날짜'].astype('str').str.replace('-', '')\n",
    "# df.to_csv(r'c:\\data\\pandas.txt', header=None, index=None, sep=' ', mode='a')\n",
    "# with open('dates_kbs.txt', 'w') as f:\n",
    "#     f.write(data['날짜_in'].astype(int).values)\n",
    "\n",
    "# data['날짜_in'].astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# nparray = data['날짜_in'].values\n",
    "# np.savetxt('dates_kbs.txt', nparray, delimiter=\",\")\n",
    "# nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.날짜_in.to_csv('dates_kbs.csv', header=None, index=None, sep=',')\n",
    "# import numpy as np\n",
    "# np.savetxt('dates_kbs.csv', data.날짜_in.values, fmt='%d')\n",
    "\n",
    "# data.날짜_in.to_csv('date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/home/sjham/project/scrapy/MyProject/data_set/dates_kbs.csv'\n",
    "a = pd.read_csv(csv_file, header=None, index_col=None)\n",
    "for i in a.values:\n",
    "    int(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data['제목_splited'] = [','.join(str(i).split()) for i in data.제목_refined]\n",
    "# data['제목_fil'] = [set(str(i).split()) for i in data.제목]\n",
    "\n",
    "# .apply(split()))\n",
    "\n",
    "# for n in data.제목_refined:\n",
    "#     a = mecab.nouns(str(n))\n",
    "    \n",
    "# [len(set(a).intersection(b)) for a, b in zip(df.A, df.B)]    \n",
    "    \n",
    "#     pd.Series([mecab.nouns(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rd in raw_data['title_splited']:\n",
    "#     str(rd)\n",
    "#     for d in data['제목_splited']:\n",
    "#         str(d)\n",
    "#         print(similar(rd, d))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter =[]\n",
    "for rd in data['제목_fil']:\n",
    "    for d in raw_data['title_fil']:\n",
    "        if len(d & rd) > 3:\n",
    "            filter.append(d)\n",
    "            \n",
    "            \n",
    "#             raw_data['filtered'] = d\n",
    "            \n",
    "        \n",
    "        \n",
    "#         raw_data.loc[len(d & rd) > 3, :]\n",
    "#         if len(d & rd) > 3:\n",
    "#             raw_data['title_splited']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({'c1': [1, 4, 7], 'c2': [2, 5, 1], 'c3': [3, 1, 1]})\n",
    "df2 = pd.DataFrame({'c4': [1, 4, 7], 'c2': [3, 5, 2], 'c3': [3, 7, 5]})\n",
    "set(df1['c2']).intersection(set(df2['c2']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(map(tuple, my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(\n",
    "    \n",
    "    set(map(tuple, my_list)), reverse=True)\n",
    "result\n",
    "result = sorted(set(map(tuple, my_list)), reverse=True)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set(raw_data['title_splited']).intersection(set(data['제목_splited']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = data.merge(raw_data, how='inner', left_on='제목', right_on='title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_merged.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
